{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7264bd-4778-4f96-a4ea-5ebb0b4d595b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#database imports\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#for accessing env\n",
    "import os \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8eb4e4b-eefc-400c-baaf-f4a90958464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "model = tf.keras.models.load_model('model/EmotionEcho_v5.keras', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c511673-25bf-489d-8b12-2a4f07b9ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database script\n",
    "\n",
    "# Database connection details, the data is stored in .env\n",
    "load_dotenv()\n",
    "DB_NAME = os.getenv(\"DB_NAME\") \n",
    "DB_USER = os.getenv(\"DB_USER\") \n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\") \n",
    "DB_HOST = os.getenv(\"DB_HOST\") \n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "\n",
    "#function to store data in database\n",
    "def store_emotion(emotion, date, time, confidence = None):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO emotions (emotion, date, time, confidence)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(insert_query, (emotion, date, time, confidence))\n",
    "        conn.commit()\n",
    "        print(f\"Data stored successfully: {emotion} at {date + time}\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf8288-3c91-4aa0-8d75-841e056ff3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Data stored successfully: surprise at 2024-12-1316:20:42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Data stored successfully: neutral at 2024-12-1316:20:46\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Data stored successfully: neutral at 2024-12-1316:20:51\n"
     ]
    }
   ],
   "source": [
    "# webcam script\n",
    "\n",
    "# Emotion classes (in the same order as during training)\n",
    "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "# Load the pre-trained Haar Cascade Classifier for face detection\n",
    "# full credit to opencv for this model that is used to recognise faces from the images\n",
    "# https://github.com/opencv/opencv/tree/master\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera; use 1 if an external webcam is used\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# font for text display\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.8\n",
    "font_thickness = 2\n",
    "rectangle_bgr = (255, 255, 255)  \n",
    "\n",
    "# Time tracking for delay\n",
    "last_processed_time = time.time()  # Initialize to the current time\n",
    "process_interval = 5  # Interval in seconds\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Get the current time\n",
    "    current_time = time.time()\n",
    "\n",
    "    if current_time - last_processed_time >= process_interval:\n",
    "        # Update the last processed time\n",
    "        last_processed_time = current_time\n",
    "    \n",
    "        # Convert frame to grayscale for Haar Cascade\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Detect faces in the image\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "        # process the detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face ROI (Region of Interest)\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "            \n",
    "            # Preprocess the face image for the model\n",
    "            face_resized = cv2.resize(face_roi, (224, 224))  # Resize to model's input size\n",
    "            face_normalized = face_resized.astype('float32') / 255.0  # Normalize pixel values to [0, 1] since this is how the model that recognises emotion was trained\n",
    "            face_input = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n",
    "    \n",
    "            # Predict the emotion\n",
    "            emotion_probabilities = model.predict(face_input)\n",
    "            emotion_index = np.argmax(emotion_probabilities)  # Get the index of the highest probability\n",
    "            predicted_emotion = classes[emotion_index]  # Map the index to the corresponding label\n",
    "            confidence = float(emotion_probabilities[0][emotion_index])\n",
    "    \n",
    "            # Save the detected emotion to the database\n",
    "            now = datetime.now()\n",
    "            store_emotion(\n",
    "                emotion=predicted_emotion,\n",
    "                date=now.strftime(\"%Y-%m-%d\"),\n",
    "                time=now.strftime(\"%H:%M:%S\"),\n",
    "                confidence=confidence\n",
    "            )\n",
    "    \n",
    "            # Draw a rectangle around the face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Blue rectangle\n",
    "            \n",
    "            # Prepare text background\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(predicted_emotion, font, font_scale, font_thickness)\n",
    "            text_x = x\n",
    "            text_y = y - 10  # Position text above the face rectangle\n",
    "            box_coords = ((text_x, text_y - text_height - 10), (text_x + text_width + 10, text_y + baseline - 10))\n",
    "            cv2.rectangle(frame, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "    \n",
    "            # Put the predicted emotion text on the frame\n",
    "            cv2.putText(frame, predicted_emotion, (text_x, text_y), font, font_scale, (0, 0, 0), font_thickness)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee7080-4190-49aa-9bd5-ddee8a17427f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
